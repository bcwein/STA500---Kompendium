\chapter{Estimering}
Når me i statistikk jobbar med sannsynlegheitsfordelingar så er det viktig å hugse på at slike sannsynlegheitsfordelingar gjelder for ein uendeleg \startsitat\textbf{populasjon}\sluttsitat og kan tenkast på som eit histogram der vi har uendelig mange utdrag. Då endrer dette \startsitat histogrammet\sluttsitat seg avhengig av parametra den tar inn. F.eks forventningsverdi og varians i ein normalfordeling.

Men her er det begrensningar. Ein populasjon i statistikken kan vere endeleg som f.eks 
\begin{itemize}
    \item Befolkningen i ein nasjon.
    \item Stjerner i mjelkevegen.
    \item Karakterar til elevane i 9B på Kvåle Skule.
\end{itemize}
Eller uendelege og konseptuelle som f.eks
\begin{itemize}
    \item Alle mulige trafikkulykker.
    \item Alle mulige lenger av eit kast.
    \item Alle mulige måter ein sjukdom kan spre seg i ein befolkning.
\end{itemize}

På grunn av populasjonens natur kan det vere vanskeleg eller umogleg å kunne komplett beskrive heile populasjonen. Likevel ønsker vi å få innsikt i populasjonen og for å gjere dette tar vi eit \textbf{utvalg} av befolkningen og estimerer verdiar i populasjonen basert på utvalget. For å få gode og gyldige tall bør ein velge eit utvalg som ikkje er \textbf{partisk}. Den enklaste måten å gjere dette på er ved å gjere utvalget tilfeldig.

\section{Statistikkar}\label{chap:statistikk}
Ein \textbf{statistikk} er i denne betydningen ein \startsitat funksjon av dei stokastiske variablane som utgjer eit tilfeldig utvalg av ein populasjon\sluttsitat. Ein statistikk er i seg sjølv ein stokastisk variabel då denne er ein funksjon av eit tilfeldig utvalg og ein populasjon kan ha mange utvalg og me forventar at slike statistikkar varierar. 

\subsection{Lokasjonsmål på eit utvalg: Utvalgsgjennomsnitt, median og typetall}

Dei vanlegaste statistikkane for å måle senter av dataen er gjennomsnitt, median og typetall. For eit utvalg data $X_1, X_2, \dots, X_n$ reknar me ut utvalgsgjennomsnittet
\begin{equation}
    \Bar{X} = \frac{1}{n}\sum_{i=1}^n X_i
\end{equation}
utvalgsmedianen
\begin{equation}
    \Tilde{x} = 
    \begin{cases}
    x_{(n+1) / 2}, & \text{Når x er oddetal}  \\
    \frac{1}{2} (x_{n/2} + x_{n/2 + 1)}, & \text{Når x er partal}
    \end{cases}
\end{equation}
og typetallet er det tallet i eit datasett som dukkar opp med høgast frekvens.

\subsection{Variasjonsmål på eit utvalg: Utvalgsvarians, standardavvik og område}
Dei vanlegaste statistikkane for å måle spredningen i dataen er varians, standardavvik og område. For eit utvalg data $X_1, X_2, \dots, X_n$ reknar me ut variansen
\begin{equation}
    S^2 = \frac{1}{n - 1} \sum_{i = 1}^n (X_i - \Bar{X})^2
\end{equation}
Som er summen av den kvadrerte avstanden fra gjennomsnittet. $n - 1$ dukker opp på grunn av at statistikken $\Bar{X}$ gjer til at vi får ein friheitsgrad mindre\ref{chap:friheitsgrad} og at estimatoren blir partisk mot utvalget om denne korreksjonen ikkje blir gjort. 

utvalgsstandardavviket kan ein tenke på som gjennomsnittlig avstand fra gjennomsnittet og blir rekna ut 
\begin{equation}
    S = \sqrt{S^2}
\end{equation}
utvalgsområdet reknar ein ut 
\begin{equation}
    R = \text{max}\{X_1, X_2, \dots, X_n\} - \text{min}\{X_1, X_2, \dots, X_n\}
\end{equation}

\section{Utvalgsfordelingar}
Som nevnt over i diskusjonen om ein \textbf{statistikk}\ref{chap:statistikk} så er denne ein funksjon av eit utvalg og sidan det er mange moglege utvalg av ein populasjon så vil denne statistikken vere ein stokastisk variabel og dermed ha ein sannsynlegheitsfordeling. Denne kallar me for ein \textbf{utvalgsfordeling}.

\subsection{Utvalgsfordelingen til \texorpdfstring{$\Bar{X}$}{Utvalgsjennomsnittet}}
På grunn av \textbf{sentralgrenseteoremet}\ref{chap:sentralgrense} så vil utvalgsgjennomsnittet gå mot ein normalfordeling uavhengig av kva den underliggande fordelingen er. Dette gjer til at når me skal gjere slutningar om eit gjennomsnitt så kan me bruke normalfordelingen til hjelp.

\subsection{Normalfordelingstilnerming av binomisk fordeling}
Sidan sentralgrenseteoremet seier at ein sum av stokastiske variablar vil gå mot ein normalfordeling når $n \rightarrow \infty$. Sidan binomisk fordeling er fordelinga du får når du summerer fleire stokastiske variablar frå ein bernoullifordeling så vil ein binomisk fordeling gå mot ein normalfordeling om antalet forsøk er stor nok. 

Eit krav vi ofte nytter for å avgjere om ein binomisk fordeling lar seg tilnermast med ein normalfordeling er
\begin{equation}
    np \geq 5
\end{equation}

\subsection{Utvalgsfordelingen til \texorpdfstring{$S^2$}{Utvalgsvariansen}}
Hvis $S^2$ er variansen til ein eit tilfeldig utvalg av størrelse $n$ frå ein normalfordelt populasjon så vil statistikken
\begin{equation}\label{eq:vardist}
    \chi^2 = \frac{(n-1)S^2}{\sigma^2} = \frac{(X_i - \Bar{X})}{\sigma^2}
\end{equation}
Ha ein $\chi^2$-fordeling med $n - 1$ friheitsgrader.

\section{t-fordelingen}
Statistikkane over tar ofte utgangspunkt i at $\sigma$ til ein populasjon er kjent. I virkelegheita er dette i mange tilfelle ikkje kjent og ein er nøydd å supplementere ein statistikk ikkje berre med utvalgsgjennomsnittet men og utvalgsstandardavviket. Det er derfor naturleg å introdusere ein statistikk for $\mu$ er då
\begin{equation}\label{eq:tstat}
    T = \frac{\Bar{X} - \mu}{S / \sqrt{n}}
\end{equation}
Og denne statistikken vil då vere t-fordelt. T-fordelingen passar bedre å bruke i tilfelle der utvalget er $n < 30$ fordi når vi har liten $n$ så har utvalgsvariansen lett for å variere for forskjellige utvalg og vi trenger ein fordeling med lengre \startsitat haler\sluttsitat enn normalfordelingen (sjå plot av t-fordeling vs normalfordeling).

Når $n > 30$ er ein tilnerming med normalfordeling stort sett godt nok og ein t-fordeling går asymptotisk mot ein normalfordeling når friheitsgraden $\nu \rightarrow \infty$.

\section{Ikkje-partiske estimatorar}
Sidan estimatorar er nettop som namnet tilseier, eit estimat. Vi ynksjer då sjølvsagt at ein estimator på ein parameter $\theta$ er nermast mogleg parameteren, med andre ord, ein estimator sin utvalgsfordeling har forventingsverdi lik $\theta$
\begin{equation}
    \mu_{\hat{\Theta}} = E[\hat{\Theta}] = \theta
\end{equation}
Statistikkar med denne eigenskapen kallar me for ikkje-partiske.

\section{Varians til punktestimator}
Estimatorar kan ha forskjellige estimatorar. Når ein estimator er ikkje-partisk og har den minste moglege variansen av alle ikkje-partiske estimatorar så er den den \textbf{mest effektive estimatoren}.

\section{Intervallestimering}
Til og med den mest effektive estimatoren vil me ikkje forvente at korrekt klarar å estimere ein parameter $\theta$. Derfor nyttar me oss av intervall estimat
\begin{equation}
    \hat{\theta_L} < \theta < \hat{\theta_U}
\end{equation}
der $\hat{\theta}_L$ og $\hat{\theta}_U$ er avhengige av ein statistikk $\boldsymbol{\hat{\Theta}}$. Me tar og hensyn til utvalgsfordelingen til statistikken $\boldsymbol{\hat{\Theta}}$.

Det er då viktig å hugse på at når vi tar eit utvalg frå ein populasjon og reknar ut statistikkane $\boldsymbol{\hat{\Theta}_L}$ og $\boldsymbol{\hat{\Theta}_U}$ (lågaste og høgste grense). Me spesifiserer desse grensene etter eit nivå av signifikans som me spesifiserar. f.eks $\alpha = 0.05$ og finner grensene sånn at
\begin{equation}
    P(\boldsymbol{\hat{\Theta_L}} < \theta < \boldsymbol{\hat{\Theta_U}}) = 1 - \alpha
\end{equation}
Der me har ein $1-\alpha$ sannsynlegheit å velge eit utvalg fra populasjonen som gir oss $\boldsymbol{\hat{\Theta}_L}$ og $\boldsymbol{\hat{\Theta}_U}$ som inneheld $\theta$. Dette kallar me for eit konfidensintervall.

\subsection{Eksempel på konfidensintervall}

\textbf{Estimere gjennomsnitt med kjent varians}

\begin{equation}
    P\left(-z_{\alpha/2} < \frac{\Bar{X} - \mu}{\sigma / \sqrt{n}} < z_{\alpha/2}\right) = 1 - \alpha
\end{equation}

Så skriver me om transformasjonen midt i utrykket til å kun innehalde parameteren og vi får

\begin{equation}
        P\left(\Bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \Bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \right) = 1 - \alpha
\end{equation}

Dette gjer vi for alle konfidensintervall. Ta ein statistikk, transformer den til ein kjent fordeling (standard normal, t, $\chi^2$) og sett ulikheiten til å kun ha parameteren av interesse i midten. Når ein skal gjere ein einsidig test så setter ein opp ulikheiten i berre ein retning og løyser for parameteren.

\textbf{Estimere gjennomsnitt med ukjent varians}

I dette tilfellet nyttar me statistikken i likning \ref{eq:tstat}, setter opp ulikheita med T verdiar og skriver om ulikheita til å isolere parameteren av interesse åleine. Da får me
\begin{equation}
    P\left(\Bar{X} - t_{\alpha/2}\frac{S}{\sqrt{n}} < \mu < \Bar{X} + t_{\alpha/2}\frac{S}{\sqrt{n}} \right) = 1 - \alpha
\end{equation}

\textbf{Estimere varians}
Me nytter oss av estimatoren og dens fordeling i likning \ref{eq:vardist} og setter opp konfidensintervallet
\begin{equation}
    P\left( \frac{(n-1)s^2}{\chi^2_{\alpha/2}} < \sigma^2 < \frac{(n-1)s^2}{\chi^2_{1 - \alpha/2}} \right) = 1 - \alpha
\end{equation}
Der $\chi^2$ har $n - 1$ friheitsgrader

\subsection{Standardfeil}
Ein statistikk har ein utvalgsfordeling og denne utvalgsfordelingen har eit standardavvik som me kallar for \textbf{standardfeilen} til estimatoren. I utvalgsgjennomsnittet sitt tilfelle så er denne stanrdardfeilen $\frac{\sigma}{\sqrt{n}}$

\subsection{Estimere ein proporsjon}
Lat oss seie at me vil estimere ein proporsjon $p$ med statistikken $\hat{P} = X/n$ er $X$ er antal velukka eksperiment i eit bernoulli eksperiment. Om me gjer fleire eksperiment og får ein verdi $x$ som er summen av velukka eksperiment av $n$ eksperiment så er $\hat{p}$ gjennomsnittet av alle eksperimenta og ved sentralgrenseteoremet vil denne statistikken vere normalfordelt med gjennomsnitt

\begin{equation}
    \mu_{\hat{p}} = E[\hat{P}] = \frac{np}{p} = p
\end{equation}

Som betyr at estimatoren er ikkje-partisk. Variansen
\begin{equation}
    \sigma^2_{\hat{p}} = \sigma^2_{X/n} = \frac{npq}{n^2} = \frac{pq}{n}
\end{equation}s

Ved å normalisere statistikken, endre på ulikheita og bruke verdiar for ein standard normalfordeling så får me
\begin{equation}
    P\left( \hat{P}-z_{\alpha/2}\sqrt{\frac{\hat{p}\hat{q}}{n}} < p < \hat{P} + z_{\alpha/2}\sqrt{\frac{\hat{p}\hat{q}}{n}} \right)
\end{equation}

\section{Rimelegheitsmaksimeringestimasjon}
\textbf{Rimelegheitsmaksimeringestimasjon} (RME) bedre kjent i faget som \textbf{Maximum Likelihood Estimation} (MLE) er ein metode for å estimere ein parameter i ein sannsynlegheitsfordeling ved hjelp av maksimering av ein rimelegheitsfunksjon for estimatoren.

\subsection{Rimeligheit og sannsynlegheit}

For å forstå RME er det viktig at ein forstår forskjellen på rimelegheit og sannsynlegheit.

\textbf{Sannsynlegheit} er ein funksjon som forteller deg at for ein gitt sannsynlegheitsfordeling $\theta$, kva er sannsynet for å observere ein observasjon $x$. Matematisk så skriver me
\begin{equation}
    p(x | \theta) = f_\theta(x)
\end{equation}
I det diskrete tilfellet. I det kontinuerlige tilfellet så nyttar me intervall som $P(X < x)$ og bruker den kumulative funksjonen $F(x)$. Det som er viktig å hugse på er at sannsynlegheit er det data $x$ som er variabelen\cite{wiki:likelihood}.

\textbf{Rimelegheit} er ein funksjon som forteller deg at gitt nokon data, kva er ein rimeleg sannsynlegheitsfordeling at denne dataen kan ha komme frå? Med andre ord: det er sannsynlegheitsfordelinga som er variabelen til rimelegheita. Matematisk skriver me
\begin{equation}
    \mathcal{L}(\theta | \boldsymbol{x}) = f_\theta(\boldsymbol{x})
\end{equation}
Der $\boldsymbol{x}$ kan vere eitt enkelt datapunkt eller ein vektor med fleire datapunkt. Sjøl om det ser ut som at det er $x$ som er variabelen her er det $\theta$ som blir tenkt på som variabelen. Rimelegheitaen er funksjonsverdien til sannsynlegheitstettleikfunksjonen eller sannsynlegheitsmassefunksjonen. 

\textbf{Eksempel}: Sjå for deg at me har $\theta \sim N(0, 1)$ og $\boldsymbol{x} = 0$. Sannsynlegheita for at me observerer data som er mindre eller lik $180$ i ein normalfordeling er $0.5$. Rimelegheita for at ein normalfordeling med snitt 0 og standardavvik 1 vil gi data som er lik $180$ er lik $f_{N(0,1)}(x) = 0.3989423$. 

RME handlar om å finne ein sannsynlegheitsfordeling for eit gitt datasett $\boldsymbol{x}$ som maksimerer rimelegheitsfunksjonen. Intuitivt vil det då vere den rimeligaste sannsynlegheitsfordelinga me kan tenke oss at dataen kjem frå.

\subsection{Framgangsmåte RME}
RME handlar enkelt og greit om at me har lyst å maksimere
\begin{equation}
    \mathcal{L}(\theta ; \boldsymbol{x})
\end{equation}
Der $\theta$ er i eit sannsynlegheitsfordelingsrom etter vårt ønske. Som f.eks ein poissonfordeling. 

Lat oss seie at i vårt tilfelle ynskjer me å finne den rimlegaste parameteren $\lambda$ av alle poissonfordelingar gitt dataen $x_1, x_2, \dots, x_n$. Rimelegheitsfunksjonen blir då

\begin{equation}
    \mathcal{L}(\lambda; x_1, x_2, \dots, x_n) = \prod_{i=1}^n f(x_i | \lambda) = \frac{e^{n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}
\end{equation}

og ved å ta logaritmen på begge sider får me

\begin{equation}
    \ln \mathcal{L}(\lambda; x_1, x_2, \dots, x_n) = -n\lambda + \sum_{i=1}^n \ln \lambda - \ln \prod_{i=1}^n x_i!
\end{equation}

deretter deriverer me med hensyn på $\lambda$

\begin{equation}
  \frac{\partial \ln \mathcal{L}(\lambda; x_1, x_2, \dots, x_n)}{\partial\lambda}  = -n + \sum_{i=1}^n \frac{x_i}{\lambda}
\end{equation}

Og setter den deriverte lik $0$ og løyser for $\lambda$ og vi får

\begin{equation}
    \hat{\lambda} = \sum_{i=1}^n \frac{x_i}{n} = \Bar{x}
\end{equation}

Og sidan den andrederiverte er eit negativt utrykk veit me at dette er eit minimumspunkt.

\subsection{RME og uforanderlighetsprinsippet}
I følge dette svaret på \href{https://stats.stackexchange.com/questions/77573/invariance-property-of-mle-what-is-the-mle-of-theta2-of-normal-barx2}{stackexchange} som baserer seg på Probability and Statistical Inference\cite{mukhopadhyay2020probability}, for ein gitt SME $\boldsymbol{\hat{\theta}}$ for parameteren $\theta$, vil det for ein kvar funksjon $f(\theta)$ vere slik at SME for $f(\theta)$ er $f(\hat{\theta})$.

Kravet for at dette skal være sant er at funksjonen $f$ må være ein ein-til-ein funksjon, formelt kalla ein \textbf{bijektiv} funksjon.

\begin{equation}
    SME: f(\theta) = f(\hat{\theta}), \qquad \text{Når f er bijektiv}
\end{equation}